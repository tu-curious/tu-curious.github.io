---
title: "In-Site Blog Post #1"
date: 2018-07-15
tags: [Machine Learning, Empirical Mean, Central Limit Theorem]
header:
excerpt: "CLT"
mathjax: "true"
permalink: /IBP_1/
author_profile: false
font-size: $type-size-8
layout: single
classes: wide
read_time: true
---


## Empirical Mean and Central Limit Theorem (CLT)


__Note 0__: If you are 
new to CLT or read it a while back, I would recommend to watch the following 
video by Khan Academy and then resume reading thereafter.

<iframe width="560" height="315" src="https://www.youtube.com/embed/JNm3M9cqWyc?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

I have heard of and used CLT in many problems but never "felt" it. This post is 
not about the proof but just an application of CLT.
I always figured it isn't the hardest thing in the world as most people understand 
it in their first or $$2^{nd}$$ attempt. But somehow, I found it hard to wrap my 
head around it. It turns out that its role is significant in understanding of 
objective functions in Machine Learning (ML). And so, I spent some time and 
finally "felt" it. This post is about the insight that I got.


I assume that the reader is familiar with the general setup of a supervised ML 
optimization problem. We observe some 
data $$(X,Y)$$ where capital letters denote they are random variables. Let's group
them together in a random variable $$Z=[X;Y]$$ (MATLAB notation, more of a slang).
We want to minimize the true expected loss, $$E[L(Z,\theta)]$$ which will be a 
function of only $$\theta$$, say $$f(\theta)$$, where \theta are the parameters 
of the ML model.

First, why do we do this?

Because this $$L$$ is a function of a random variable $$Z$$ and hence a random variable 
itself. The best we can do, in minimization of any random quantity, is to minimize 
its expected value. Hence we minimize $$E[L]$$ where expectation is over the 
random variable $$Z$$ or the joint pdf(probability density function) of $$(X,Y)$$. Furthermore, we need to minimize 
the true $$E[L]$$ but what we can practically get is a finite set of samples and 
hence only the sample/empirical mean, $$E^{\hat}[L]=(1/n)\times\sigma_{i=1}^nl_i$$, where
$$n$$ is no. of samples and $$l_i$$ refers to value of $$L$$ for $$i^th$$ data point.
(Refer to [this](http://www.deeplearningbook.org/contents/optimization.html) for more details). 
Now here is where CLT starts to come in. We can 
intuitively feel that if we were given infinite or all possible samples of $$L$$, 
we would get the true mean $$E[L]$$. But because we need to work with the finite 
empirical mean, __**is it even a good approximation of the true mean?**__

The essential part is to realize that this empirical mean is itself a random 
variable (say $$B=E^{\hat}[L]$$) because it's just a scaled sum of random variables 
$$Z$$. But what is the pdf of $$B$$. CLT tells us that for large enough no. of samples, $$n$$, 
$$B~\mathcal{N}(\mu,\sigma^2)$$ . There it is! The empirical mean we calculate 
for any given dataset is nothing but a point sampled from a gaussian distribution 
whose mean $$\mu$$ is the true mean $$E[L]$$ we are seeking and variance $$\sigma^2$$ 
is $$\frac{var(L)}{n}$$. So, according to CLT, as n increases, this is what starts 
happening to the pdf of $$B$$.

** picture of gaussian reducing variance and turning into a delta using MATLAB\spyder**
<img src="https://tushar-agarwal2909.github.io/images/CLT1.gif" width="400" height="400" align="middle" />


This figure is the pictorial representation of a more general phenomenon which 
we all have heard about, **the convergence**. So, empirical mean is just a point 
from this pdf. Therefore, as we can say for any sample from a gaussian pdf, we are 99% 
confident that this one point (we sampled) is within $$\pm3\sigma$$ of the true 
value we were seeking in the first place. Hence, the variance refers to the 
error in our estimate.

The bound can then be formally written as

$$P(|B-E[L]|<(3\sqrt{\frac{var(L)}{n}}))>0.9973$$


__Note 1__: Same concept can be extended to the gradient estimate after exchanging 
the order of application of the gradient and the expectation operator (due to linearity 
of both operators), i.e.

$$\nabla_{\theta}E[L]=E[\nabla_{\theta}L]=E[G(Z,\theta)]$$
where point-wise gradient $$G$$ is just another function like $$L$$ and same things apply while 
calculating empirical expectations of gradients, whether on a batch or a mini-batch.

__Note 2__: This is a very basic perspective to the convergence question using CLT. 
There has been **!A LOT!** of research and proofs that follow, using advanced 
concepts/theorems which give better bounds. But this can be considered as a 
first step (I guess) towards that theory.

Hope this will help somebody, somewhere, someday.


-Tushar