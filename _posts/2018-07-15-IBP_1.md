---
title: "In-Site Blog Post #1"
date: 2018-07-15
tags: [Machine Learning, Empirical Mean, Central Limit Theorem]
header:
excerpt: "CLT"
mathjax: "true"
permalink: /IBP_1/
author_profile: false
font-size: $type-size-8
layout: single
classes: wide
read_time: true
---


## Empirical Mean and Central Limit Theorem (CLT)


I have heard of and used CLT in many problems but never "felt" it. If you are 
new to CLT or read it a while back, I would recommend to watch the following 
video by Khan Academy and then resume reading thereafter.

<iframe width="854" height="480" src="https://www.youtube.com/watch?v=JNm3M9cqWyc" frameborder="0" allowfullscreen></iframe>

This post is not about the proof but just an application of CLT.
I always felt it isn't the hardest thing in the world as most people understand 
it in their first or $$2^nd$$ attempt. But somehow, I found it hard to wrap my 
head around it. It turns out that its role is significant in understanding of 
objective functions in Machine Learning (ML). And so, I spent some time and 
finally "felt" it. This post is about the insight that I got.


I assume that the reader is familiar with the general setup of a supervised ML 
optimization problem. What is happening in the context of ML. We observe some 
data $$(X,Y)$$ where capital letters denote they are random variables. Let's group
them together in a random variable $$Z=[X;Y]$$ (MATLAB notation, more of a slang.
We want to minimize the true Expected loss, $$E[L(Z,\theta)]$$ which will be a 
function of only $$\theta$$, say $$f(\theta)$$.

First, why do we do this?

Because this L is a function of a random variable $$Z$$ and hence a random variable 
itself. The best we can do, in minimization of any random quantity, is to minimize 
its expected value. Hence we minimize $$E[L]$$. Furthermore, we need to minimize 
the true $$E[L]$$ but what we can practically get is a finite set of samples and 
hence only the sample/empirical mean. (Refer to [this](http://www.deeplearningbook.org/contents/optimization.html) for more detail). 
Now here is where CLT starts to come in. We can 
intuitively feel that if we were given infinite or all possible samples of L, 
we would get the true mean. But because we need to work with the finite 
empirical mean, is it even a good approximation of the true mean.

The essential part is to realize that this empirical mean is itself a random 
variable (say $$B$$) because it's just a scaled sum of random variables Z. But what 
is the pdf of $$B$$. CLT tells us that for large enough no. of samples, say $$n$$, 
$$B~\script{N}(\mu,\sigma^2)$$ . There it is! The empirical mean we calculate 
for any given dataset is nothing but a point sampled from a gaussian distribution 
whose mean $$\mu$$ is the true mean $$E[L]$$ we are seeking and variance $$\sigma^2$$ 
is $$var(L)/n$$. So, according to CLT, as n increases, this is what starts 
happening to the pdf of $$B$$.

** picture of gaussian reducing variance and turning into a delta using MATLAB\spyder**
<img src="https://tushar-agarwal2909.github.io/images/CLT1.gif" width="400" height="400" />


This figure is the pictorial representation of a more general phenomenon which 
we all have heard about, **the convergence**. So, empirical mean is just a point 
from this pdf and as we can say for any sample from a gaussian pdf, we are 99% 
confident that this one point we sampled is within $$+-3\sigma$$ of the true 
value we were seeking in the first place. Hence, the variance refers to the 
error in our estimate.

The bound can then be formally written as

$$P(|B-E[L]|<(3\sqrt{var(L)/n}))>0.9973$$


Note: Same concept can be extended to the gradient estimate after exchanging 
the order of application of gradient and expectation operator (due to linearity 
of both operators), i.e.

$$\nabla_{\theta}E[L]=E[\nabla_{\theta}L]=E[G(Z,\theta)]$$
where gradient G is just another function like L and same things apply while 
calculating empirical expectations of gradients, whether on a batch or mini-batch.

Note 2: This is a very basic perspective to the convergence question using CLT. 
There has been **!A LOT!** of research and proofs that follow, using other 
concepts/theorems which give better bounds. But this can be considered the 
first baby step (I guess) towards that theory.

Hope this will help somebody, somewhere, someday.


-Tushar