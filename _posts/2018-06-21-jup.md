
## Heading

I am trying out Jupyter to webpage conversion using jekyll.


```python
import keras as kr
```

---
title: "Title"
author: "Me"
header-includes:
   - \usepackage{bbm}
output:
    pdf_document
---

Let us try some latex. $56+x+c+\pi$

$$d+\phi^2_3$$

\begin{equation}
d+\phi^2_3
\end{equation}

~~strike~~. **bold**
```python
import stuff
print(stuff)```

\begin{document}
\begin{center}
\begin{Large}
\textbf{A Self-driving car simulation using Q-Learning}\\
\end{Large}
\vspace{2mm}
\begin{small}
\textbf{Tushar Agarwal} (agarwal.270@osu.edu), \textbf{Wenxiao Zhan} (zhan.137@osu.edu)\\
\end{small}
\end{center}

\section*{Introduction}

Creating intelligent machines has long been a human endeavor with a rich past as well as present. Reinforcement learning is one major component of this endeavor. As the name suggests, reinforcement learning refers to a reward based learning system designed for a machine to teach it the correct behavior of acting in a specified environment. It has its roots in behaviorist psychology and can be simply thought of as the reward based training routine usually followed while teaching pet animals like dogs. Such a training requires a time-dependent modeling of the processes. \\

\subsection*{Components}
Important components of an RL agent are the policy, the value function and the model.
\begin{itemize}

\item The model predicts how the environment will behave next. It has 2 main sub-components:-\\
The transition probability: $\mathcal{P}(s,a,s')=P(s_{t+1}=s'|s_{t}=s,a_{t}=a)$\\
The Reward: $r(s,a)=E[r_{t+1}|s_{t}=s,a_{t}=a]$

\item The policy $\pi$ defines the agents behavior and is a mapping from state $s_t$ to action $a_t$. It can be be deterministic or probabilistic.\\
Deterministic policy: $a_t=\pi(s_t)$\\
Probabilistic policy: $\pi(a|s)=P(a_t=a|s_t=s)$

\item Value function is a prediction of the future reward. It tries to measure the goodness or badness of states $V(s_t)$ or state-action pairs $q(s_t,a_t)$ to help make a decision between available set of actions $A$ at a given state.\\
State value function for a given policy $\pi$: $V_{\pi}(s)=E_{\pi}[G_t|s_t=s]$\\
State-action value function for a given policy $\pi$: $Q_{\pi}(s,a)=E_{\pi}[G_t|s_t=s,a_t=a]$\\

where $G_t=r_t+\sum_{i>t}\gamma^{i-t}r_i$ is a measure of the cumulative reward.
The term $\gamma \in [0,1)$ is a discount factor used to represent the fact that the present reward is more valuable than the future reward as the future is always uncertain. This is a very practical assumption and has real life manifestations like time value of money in finances. It also allows the cumulative reward to be finite in case of infinite time horizon, i.e. when the agent takes actions forever.

\end{itemize}

